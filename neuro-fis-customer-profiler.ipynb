{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuro-Fuzzy System for Customer 360 Profiling and Churn Prediction\n",
    "\n",
    "A 360-degree enriched customer profile is key to derive actionable insights and smarter data-driven decisions.\n",
    "\n",
    "Customer profile enrichment can be applied across multiple business use cases:\n",
    "\n",
    "- Design targeted sales events.\n",
    "- Personalize promotions and offers\n",
    "- Proactively reduce customer churn\n",
    "- Predict customer buying behavior\n",
    "- Forecast demands for better pricing and inventory\n",
    "- Gain actionable insights for better sales and opportunity leads\n",
    "\n",
    "Some of the benefits of customer profile enrichment includes:\n",
    "\n",
    "- A 360-degree representation of the customer.\n",
    "- Holistic view of your ideal customersâ€™ purchasing behaviors and patterns.\n",
    "- Allows you to locate, understand and better connect to your ideal customers.\n",
    "\n",
    "### Example for car sharing fleet management and maintenance tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from obs import *\n",
    "import csv\n",
    "import io\n",
    "import time\n",
    "# imports for fuzzy inference system in churn prediction\n",
    "import numpy as np\n",
    "import skfuzzy as fuzz\n",
    "from skfuzzy import control as ctrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load first dataset\n",
    "dataPairIdentifier1 = 'customer_nrActiveCarsInFleet_nrServiceComplaints.csv';\n",
    "\n",
    "# load the second dataset \n",
    "dataPairIdentifier2 = 'customer_nrActiveCarsInFleet_nrMaintenanceTickets.csv';\n",
    "\n",
    "# load the third dataset \n",
    "dataPairIdentifier3 = 'customer_nrServiceComplaints_nrMaintenanceTickets.csv';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the value of the logistic function for single neuron dynamics\n",
    "# given the slope, m, and the shift, s\n",
    "def compute_s(x, m ,s):\n",
    "    y = np.zeros((len(x), 1))\n",
    "    for idx in range(len(x)):\n",
    "        y[idx, 0] = 1/(1 + np.exp(-m*(x[idx, 0] - s)))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute topological distance between activations in each of the neurons\n",
    "def compute_d(N, s):\n",
    "    y = np.zeros((N, N))\n",
    "    for idx in range(N):\n",
    "        for jdx in range(N):\n",
    "            y[idx, jdx] = np.exp(-0.5*(np.min([np.abs(idx-jdx), N - np.abs(idx-jdx)])/s) ** 2)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the learning network composed of N_POP populations of\n",
    "# N_NEURONS neurons and init each struct weight and activity matrices\n",
    "def create_init_network(N_POP, N_NEURONS, GAMMA, SIGMA, DELTA, MAX_INIT_RANGE, TARGET_VAL_ACT):\n",
    "    wta_profile = GAMMA * compute_d(N_NEURONS, SIGMA) - DELTA;\n",
    "    wext = np.random.rand(N_NEURONS, N_NEURONS)*MAX_INIT_RANGE;\n",
    "    populations = {}\n",
    "    for pop_idx in range(N_POP):\n",
    "        populations[pop_idx] = {'idx': pop_idx,\n",
    "                                'lsize': N_NEURONS,\n",
    "                                'Wint': wta_profile,\n",
    "                                'Wext': wext / np.sum(wext),\n",
    "                                'a': np.zeros((N_NEURONS, 1))*TARGET_VAL_ACT,\n",
    "                                'h': np.zeros((N_NEURONS, 1))*TARGET_VAL_ACT\n",
    "                               }\n",
    "    return populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate the population encoded variable as input for the net\n",
    "# here we also need to encode variables which are in both +/- ranges\n",
    "# we need to take into accound the encoding for the tuning curves\n",
    "# distribution\n",
    "def population_encoder(x, range_input, N):\n",
    "    sig = .1  # Standard deviation\n",
    "    K = 1; # max firing rate (Hz) (ignore - not modeling nurophysiology here :)\n",
    "    # pattern of activity, or output tuning curve between [-range, range]\n",
    "    R = np.zeros((N, 1))\n",
    "    # calculate output \n",
    "    for j in range(N):  # for each neuron in the population\n",
    "        R[j, 0] = K*np.exp( -(x - (-range_input+(j)*(range_input/((N)/2))))**2 / (2*sig**2))\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a heatmap \n",
    "def print_heatmap(arr):\n",
    "    plt.figure()\n",
    "    plt.imshow(arr)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzzy Inference Engine\n",
    "To describe the business logic in customer profiling and churning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "dataActiveCarsInFleet = trainDataChurn1\n",
    "dataMaintenanceTickets = trainDataChurn2\n",
    "\n",
    "# random seed init\n",
    "np.random.seed(0)\n",
    "# build the fuzzy inference variables\n",
    "# inputs\n",
    "nrActiveCarsInFleet = ctrl.Antecedent(universe=np.arange(min(dataActiveCarsInFleet), max(dataActiveCarsInFleet), 1), label='nrActiveCarsInFleet')\n",
    "nrMaintenanceTickets = ctrl.Antecedent(universe=np.arange(min(dataMaintenanceTickets), max(dataMaintenanceTickets), 1), label='nrMaintenanceTickets')\n",
    "# output\n",
    "churnProbability = ctrl.Consequent(universe=np.arange(0, 101, 1), label='churnProbability')\n",
    "\n",
    "# parametrize the variables membership functions\n",
    "# inputs\n",
    "nrActiveCarsInFleet.automf(number=3, variable_type='number', names=['low', 'medium', 'high'])\n",
    "nrMaintenanceTickets.automf(number=3, variable_type='number', names=['low', 'medium', 'high'])\n",
    "# output\n",
    "churnProbability['low'] = fuzz.trimf(x=churnProbability.universe, abc=[0, 0, 50])\n",
    "churnProbability['medium'] = fuzz.trimf(x=churnProbability.universe, abc=[0, 50, 100])\n",
    "churnProbability['high'] = fuzz.trimf(x=churnProbability.universe, abc=[50, 100, 100])\n",
    "\n",
    "\n",
    "# add the BI rules for churning\n",
    "rule1 = ctrl.Rule(\n",
    "    antecedent=nrActiveCarsInFleet['low'] & nrMaintenanceTickets['low'],\n",
    "    consequent=churnProbability['low']\n",
    ")\n",
    "\n",
    "rule2 = ctrl.Rule(\n",
    "    antecedent=nrActiveCarsInFleet['low'] & nrMaintenanceTickets['medium'],\n",
    "    consequent=churnProbability['medium']\n",
    ")\n",
    "\n",
    "rule3 = ctrl.Rule(\n",
    "    antecedent=nrActiveCarsInFleet['medium'] & nrMaintenanceTickets['low'],\n",
    "    consequent=churnProbability['low']\n",
    ")\n",
    "\n",
    "rule4 = ctrl.Rule(\n",
    "    antecedent=nrActiveCarsInFleet['medium'] & nrMaintenanceTickets['medium'],\n",
    "    consequent=churnProbability['medium']\n",
    ")\n",
    "\n",
    "rule5 = ctrl.Rule(\n",
    "    antecedent=nrActiveCarsInFleet['high'] & nrMaintenanceTickets['low'],\n",
    "    consequent=churnProbability['low']\n",
    ")\n",
    "\n",
    "rule6 = ctrl.Rule(\n",
    "    antecedent=nrActiveCarsInFleet['low'] & nrMaintenanceTickets['high'],\n",
    "    consequent=churnProbability['high']\n",
    ")\n",
    "\n",
    "\n",
    "rule7 = ctrl.Rule(\n",
    "    antecedent=nrActiveCarsInFleet['medium'] & nrMaintenanceTickets['high'],\n",
    "    consequent=churnProbability['high']\n",
    ")\n",
    "\n",
    "\n",
    "# build the inference engine\n",
    "churnProbability_ctrl = ctrl.ControlSystem(rules=[rule1, rule2, rule3, rule4, rule5, rule6, rule7])\n",
    "\n",
    "# build \n",
    "churnProbability = ctrl.ControlSystemSimulation(control_system=churnProbability_ctrl)\n",
    "churnProbability.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network to learn pairwise relations between the features of the customer. \n",
    "The final 360 profile is based on the extracted relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INIT SIMULATION \n",
    "# enables dynamic visualization on network runtime\n",
    "DYN_VISUAL      = 1\n",
    "# number of populations in the network\n",
    "N_POP           = 2\n",
    "# number of neurons in each population\n",
    "N_NEURONS       = 200\n",
    "# max range value @ init for weights and activities in the population\n",
    "MAX_INIT_RANGE  = 1\n",
    "# WTA circuit settling threshold\n",
    "EPSILON         = 1e-3\n",
    "## INIT NETWORK DYNAMICS\n",
    "# epoch iterator in outer loop (HL, HAR)\n",
    "t       = 1;\n",
    "# network iterator in inner loop (WTA)\n",
    "tau     = 1;\n",
    "# constants for WTA circuit (convolution based WTA), these will provide a\n",
    "# profile peaked at ~ TARGET_VAL_ACT\n",
    "DELTA   = -0.005                   # displacement of the convolutional kernel (neighborhood)\n",
    "SIGMA   = 5.0                      # standard deviation in the exponential update rule\n",
    "SL      = 4.5                      # scaling factor of neighborhood kernel\n",
    "GAMMA   = SL/(SIGMA*np.sqrt(2*np.pi))    # convolution scaling factor\n",
    "# constants for Hebbian linkage\n",
    "ALPHA_L = 1.0*1e-2                 # Hebbian learning rate\n",
    "ALPHA_D = 1.0*1e-2                 # Hebbian decay factor ALPHA_D >> ALPHA_L\n",
    "# constants for HAR\n",
    "C       = 0.005                    # scaling factor in homeostatic activity regulation\n",
    "TARGET_VAL_ACT  = 0.4              # amplitude target for HAR\n",
    "A_TARGET        = TARGET_VAL_ACT*np.ones((N_NEURONS, 1)) # HAR target activity vector\n",
    "# constants for neural units in neural populations\n",
    "M       = 1; # slope in logistic function @ neuron level\n",
    "S       = 10.0; # shift in logistic function @ neuron level\n",
    "# activity change weight (history vs. incoming knowledge)\n",
    "ETA     = 0.25;\n",
    "\n",
    "## SENSORY DATA SETUP \n",
    "\n",
    "# prepare first dataset\n",
    "# bind sensory data\n",
    "sensory_data1 = trainData1;\n",
    "\n",
    "# get data len\n",
    "DATASET_LEN     = len(sensory_data1[:, 0])\n",
    "\n",
    "# prepare second dataset\n",
    "# bind sensory data\n",
    "sensory_data2 = trainData2;\n",
    "\n",
    "# prepare third dataset\n",
    "# bind sensory data\n",
    "sensory_data3 = trainData3;\n",
    "\n",
    "# churn history\n",
    "churn_history = np.zeros((DATASET_LEN, 1))\n",
    "\n",
    "## CREATE NETWORK AND INITIALIZE\n",
    "\n",
    "# create network for the first dataset\n",
    "# create a network given the simulation constants\n",
    "populations1 = create_init_network(N_POP, N_NEURONS, GAMMA, SIGMA, DELTA, MAX_INIT_RANGE, TARGET_VAL_ACT)\n",
    "# buffers for changes in activity in WTA loop\n",
    "act1 = np.zeros((N_NEURONS, N_POP))*MAX_INIT_RANGE\n",
    "old_act1 = np.zeros((N_NEURONS, N_POP))*MAX_INIT_RANGE\n",
    "# buffers for running average of population activities in HAR loop\n",
    "old_avg1 = np.zeros((N_POP, N_NEURONS))\n",
    "cur_avg1 = np.zeros((N_POP, N_NEURONS))\n",
    "# the new rate values\n",
    "delta_a0_1 = np.zeros((N_NEURONS, 1))\n",
    "delta_a1_1 = np.zeros((N_NEURONS, 1))\n",
    "tau1 = tau\n",
    "\n",
    "# create network for the second dataset\n",
    "# create a network given the simulation constants\n",
    "populations2 = create_init_network(N_POP, N_NEURONS, GAMMA, SIGMA, DELTA, MAX_INIT_RANGE, TARGET_VAL_ACT)\n",
    "# buffers for changes in activity in WTA loop\n",
    "act2 = np.zeros((N_NEURONS, N_POP))*MAX_INIT_RANGE\n",
    "old_act2 = np.zeros((N_NEURONS, N_POP))*MAX_INIT_RANGE\n",
    "# buffers for running average of population activities in HAR loop\n",
    "old_avg2 = np.zeros((N_POP, N_NEURONS))\n",
    "cur_avg2 = np.zeros((N_POP, N_NEURONS))\n",
    "# the new rate values\n",
    "delta_a0_2 = np.zeros((N_NEURONS, 1))\n",
    "delta_a1_2 = np.zeros((N_NEURONS, 1))\n",
    "tau2 = tau\n",
    "\n",
    "# create network for the third dataset\n",
    "# create a network given the simulation constants\n",
    "populations3 = create_init_network(N_POP, N_NEURONS, GAMMA, SIGMA, DELTA, MAX_INIT_RANGE, TARGET_VAL_ACT)\n",
    "# buffers for changes in activity in WTA loop\n",
    "act3 = np.zeros((N_NEURONS, N_POP))*MAX_INIT_RANGE\n",
    "old_act3 = np.zeros((N_NEURONS, N_POP))*MAX_INIT_RANGE\n",
    "# buffers for running average of population activities in HAR loop\n",
    "old_avg3 = np.zeros((N_POP, N_NEURONS))\n",
    "cur_avg3 = np.zeros((N_POP, N_NEURONS))\n",
    "# the new rate values\n",
    "delta_a0_3 = np.zeros((N_NEURONS, 1))\n",
    "delta_a1_3 = np.zeros((N_NEURONS, 1))\n",
    "tau3 = tau\n",
    "\n",
    "## NETWORK SIMULATION LOOP\n",
    "# # present each entry in the dataset for MAX_EPOCHS epochs to train the net\n",
    "for didx in range(DATASET_LEN):\n",
    "      \n",
    "    # loop for the first dataset\n",
    "    # pick a new sample from the dataset and feed it to the input (noiseless input)\n",
    "    # population in the network (in this case X -> A -> | <- B <- Y)\n",
    "    X1 = population_encoder(sensory_data1[didx, 0], np.max(sensory_data1[:, 0]),  N_NEURONS)\n",
    "    Y1 = population_encoder(sensory_data1[didx, 1], np.max(sensory_data1[:, 1]),  N_NEURONS)\n",
    "    # normalize input such that the activity in all units sums to 1.0\n",
    "    X1 /= np.sum(X1)\n",
    "    Y1 /= np.sum(Y1)\n",
    "    # clamp input to neural populations\n",
    "    populations1[0]['a'] = X1\n",
    "    populations1[1]['a'] = Y1\n",
    "    # given the input sample wait for WTA circuit to settle and then\n",
    "    # perform a learning step of Hebbian learning and HAR\n",
    "    while True:\n",
    "        # compute changes in activity\n",
    "        delta_a0_1 = compute_s(populations1[0]['h'] + np.matmul(populations1[0]['Wext'], populations1[1]['a']) + \n",
    "                             np.matmul(populations1[0]['Wint'],populations1[0]['a']), \n",
    "                             M, \n",
    "                             S)\n",
    "        delta_a1_1 = compute_s(populations1[1]['h'] + np.matmul(populations1[1]['Wext'], populations1[0]['a']) + \n",
    "                             np.matmul(populations1[1]['Wint'], populations1[1]['a']), \n",
    "                             M, \n",
    "                             S)\n",
    "        # update the activities of each population\n",
    "        populations1[0]['a'] = (1-ETA)*populations1[0]['a'] + ETA*delta_a0_1\n",
    "        populations1[1]['a'] = (1-ETA)*populations1[1]['a'] + ETA*delta_a1_1\n",
    "        # current activation values holder\n",
    "        for pop_idx in range(N_POP):\n",
    "            act1[:, pop_idx] = populations1[pop_idx]['a'].reshape(-1)\n",
    "        # check if activity has settled in the WTA loop\n",
    "        q1 = (np.sum(np.sum(np.abs(act1 - old_act1)))/(N_POP*N_NEURONS))\n",
    "        if q1 <= EPSILON:\n",
    "            tau1 = 1\n",
    "            break\n",
    "        # update history of activities\n",
    "        old_act1 = act1\n",
    "        # increment time step in WTA loop\n",
    "        tau1 += 1\n",
    "    # update Hebbian linkage between the populations (decaying Hebbian rule)\n",
    "    populations1[0]['Wext'] = (1-ALPHA_D)*populations1[0]['Wext'] + ALPHA_L*np.matmul(populations1[0]['a'],populations1[1]['a'].transpose())\n",
    "    populations1[1]['Wext'] = (1-ALPHA_D)*populations1[1]['Wext'] + ALPHA_L*np.matmul(populations1[1]['a'],populations1[0]['a'].transpose())\n",
    "    # compute the inverse time for exponential averaging of HAR activity\n",
    "    omegat1 = 0.002 + 0.998/(t+2)\n",
    "    # for each population in the network\n",
    "    for pop_idx in range(N_POP):\n",
    "        # update Homeostatic Activity Regulation terms\n",
    "        # compute exponential average of each population at current step\n",
    "        cur_avg1[pop_idx, :] = (1-omegat1)*old_avg1[pop_idx, :] + omegat1*populations1[pop_idx]['a'].transpose()\n",
    "        # update homeostatic activity terms given current and target act.\n",
    "        populations1[pop_idx]['h'] = populations1[pop_idx]['h'] + C*(TARGET_VAL_ACT - cur_avg1[pop_idx, :].transpose())\n",
    "    # update averging history\n",
    "    old_avg1 = cur_avg1\n",
    "    \n",
    "    # loop for the second dataset\n",
    "    # pick a new sample from the dataset and feed it to the input (noiseless input)\n",
    "    # population in the network (in this case X -> A -> | <- B <- Y)\n",
    "    X2 = population_encoder(sensory_data2[didx, 0], np.max(sensory_data2[:, 0]),  N_NEURONS)\n",
    "    Y2 = population_encoder(sensory_data2[didx, 1], np.max(sensory_data2[:, 1]),  N_NEURONS)\n",
    "    # normalize input such that the activity in all units sums to 1.0\n",
    "    X2 /= np.sum(X2)\n",
    "    Y2 /= np.sum(Y2)\n",
    "    # clamp input to neural populations\n",
    "    populations2[0]['a'] = X2\n",
    "    populations2[1]['a'] = Y2\n",
    "    # given the input sample wait for WTA circuit to settle and then\n",
    "    # perform a learning step of Hebbian learning and HAR\n",
    "    while True:\n",
    "        # compute changes in activity\n",
    "        delta_a0_2 = compute_s(populations2[0]['h'] + np.matmul(populations2[0]['Wext'], populations2[1]['a']) + \n",
    "                             np.matmul(populations2[0]['Wint'],populations2[0]['a']), \n",
    "                             M, \n",
    "                             S)\n",
    "        delta_a1_2 = compute_s(populations2[1]['h'] + np.matmul(populations2[1]['Wext'], populations2[0]['a']) + \n",
    "                             np.matmul(populations2[1]['Wint'], populations2[1]['a']), \n",
    "                             M, \n",
    "                             S)\n",
    "        # update the activities of each population\n",
    "        populations2[0]['a'] = (1-ETA)*populations2[0]['a'] + ETA*delta_a0_2\n",
    "        populations2[1]['a'] = (1-ETA)*populations2[1]['a'] + ETA*delta_a1_2\n",
    "        # current activation values holder\n",
    "        for pop_idx in range(N_POP):\n",
    "            act2[:, pop_idx] = populations2[pop_idx]['a'].reshape(-1)\n",
    "        # check if activity has settled in the WTA loop\n",
    "        q2 = (np.sum(np.sum(np.abs(act2 - old_act2)))/(N_POP*N_NEURONS))\n",
    "        if q2 <= EPSILON:\n",
    "            tau2 = 1\n",
    "            break\n",
    "        # update history of activities\n",
    "        old_act2 = act2\n",
    "        # increment time step in WTA loop\n",
    "        tau2 += 1\n",
    "    # update Hebbian linkage between the populations (decaying Hebbian rule)\n",
    "    populations2[0]['Wext'] = (1-ALPHA_D)*populations2[0]['Wext'] + ALPHA_L*np.matmul(populations2[0]['a'],populations2[1]['a'].transpose())\n",
    "    populations2[1]['Wext'] = (1-ALPHA_D)*populations2[1]['Wext'] + ALPHA_L*np.matmul(populations2[1]['a'],populations2[0]['a'].transpose())\n",
    "    # compute the inverse time for exponential averaging of HAR activity\n",
    "    omegat2 = 0.002 + 0.998/(t+2)\n",
    "    # for each population in the network\n",
    "    for pop_idx in range(N_POP):\n",
    "        # update Homeostatic Activity Regulation terms\n",
    "        # compute exponential average of each population at current step\n",
    "        cur_avg2[pop_idx, :] = (1-omegat2)*old_avg2[pop_idx, :] + omegat2*populations2[pop_idx]['a'].transpose()\n",
    "        # update homeostatic activity terms given current and target act.\n",
    "        populations2[pop_idx]['h'] = populations2[pop_idx]['h'] + C*(TARGET_VAL_ACT - cur_avg2[pop_idx, :].transpose())\n",
    "    # update averging history\n",
    "    old_avg2 = cur_avg2\n",
    "    \n",
    "    # loop for the third dataset\n",
    "    # pick a new sample from the dataset and feed it to the input (noiseless input)\n",
    "    # population in the network (in this case X -> A -> | <- B <- Y)\n",
    "    X3 = population_encoder(sensory_data3[didx, 0], np.max(sensory_data3[:, 0]),  N_NEURONS)\n",
    "    Y3 = population_encoder(sensory_data3[didx, 1], np.max(sensory_data3[:, 1]),  N_NEURONS)\n",
    "    # normalize input such that the activity in all units sums to 1.0\n",
    "    X3 /= np.sum(X3)\n",
    "    Y3 /= np.sum(Y3)\n",
    "    # clamp input to neural populations\n",
    "    populations3[0]['a'] = X3\n",
    "    populations3[1]['a'] = Y3\n",
    "    # given the input sample wait for WTA circuit to settle and then\n",
    "    # perform a learning step of Hebbian learning and HAR\n",
    "    while True:\n",
    "        # compute changes in activity\n",
    "        delta_a0_3 = compute_s(populations3[0]['h'] + np.matmul(populations3[0]['Wext'], populations3[1]['a']) + \n",
    "                             np.matmul(populations3[0]['Wint'],populations3[0]['a']), \n",
    "                             M, \n",
    "                             S)\n",
    "        delta_a1_3 = compute_s(populations3[1]['h'] + np.matmul(populations3[1]['Wext'], populations3[0]['a']) + \n",
    "                             np.matmul(populations3[1]['Wint'], populations3[1]['a']), \n",
    "                             M, \n",
    "                             S)\n",
    "        # update the activities of each population\n",
    "        populations3[0]['a'] = (1-ETA)*populations3[0]['a'] + ETA*delta_a0_3\n",
    "        populations3[1]['a'] = (1-ETA)*populations3[1]['a'] + ETA*delta_a1_3\n",
    "        # current activation values holder\n",
    "        for pop_idx in range(N_POP):\n",
    "            act3[:, pop_idx] = populations3[pop_idx]['a'].reshape(-1)\n",
    "        # check if activity has settled in the WTA loop\n",
    "        q3 = (np.sum(np.sum(np.abs(act3 - old_act3)))/(N_POP*N_NEURONS))\n",
    "        if q3 <= EPSILON:\n",
    "            tau3 = 1\n",
    "            break\n",
    "        # update history of activities\n",
    "        old_act3 = act3\n",
    "        # increment time step in WTA loop\n",
    "        tau3 += 1\n",
    "    # update Hebbian linkage between the populations (decaying Hebbian rule)\n",
    "    populations3[0]['Wext'] = (1-ALPHA_D)*populations3[0]['Wext'] + ALPHA_L*np.matmul(populations3[0]['a'],populations3[1]['a'].transpose())\n",
    "    populations3[1]['Wext'] = (1-ALPHA_D)*populations3[1]['Wext'] + ALPHA_L*np.matmul(populations3[1]['a'],populations3[0]['a'].transpose())\n",
    "    # compute the inverse time for exponential averaging of HAR activity\n",
    "    omegat3 = 0.002 + 0.998/(t+2)\n",
    "    # for each population in the network\n",
    "    for pop_idx in range(N_POP):\n",
    "        # update Homeostatic Activity Regulation terms\n",
    "        # compute exponential average of each population at current step\n",
    "        cur_avg3[pop_idx, :] = (1-omegat3)*old_avg3[pop_idx, :] + omegat3*populations3[pop_idx]['a'].transpose()\n",
    "        # update homeostatic activity terms given current and target act.\n",
    "        populations3[pop_idx]['h'] = populations3[pop_idx]['h'] + C*(TARGET_VAL_ACT - cur_avg3[pop_idx, :].transpose())\n",
    "    # update averging history\n",
    "    old_avg3 = cur_avg3\n",
    "    \n",
    "    # increment timestep for HL and HAR loop\n",
    "    t = t + 1\n",
    "    \n",
    "    # fuzzy inference for churn probability calculation\n",
    "    churnProbability.inputs({\n",
    "        'nrActiveCarsInFleet': dataActiveCarsInFleet[didx],\n",
    "        'nrMaintenanceTickets': dataMaintenanceTickets[didx]\n",
    "    })\n",
    "    churnProbability.compute()\n",
    "    churn_history[didx] = churnProbability.output['churnProbability']\n",
    "    \n",
    "    # give time to write to update DLV\n",
    "    time.sleep(2)\n",
    "    # write inputs\n",
    "    write_in(didx, np.matrix(sensory_data1[0:didx, :]), dataPairIdentifier1)\n",
    "    write_in(didx, np.matrix(sensory_data2[0:didx, :]), dataPairIdentifier2)\n",
    "    write_in(didx, np.matrix(sensory_data3[0:didx, :]), dataPairIdentifier3)\n",
    "    # write results\n",
    "    write_out(populations1[1]['Wext'], dataPairIdentifier1)\n",
    "    write_out(populations2[1]['Wext'], dataPairIdentifier2)\n",
    "    write_out(populations3[1]['Wext'], dataPairIdentifier3)\n",
    "    # write churn probability\n",
    "    write_churn(didx, churn_history[0:didx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print to verify the learnt relations\n",
    "print_heatmap(populations1[1]['Wext']); print_heatmap(populations2[1]['Wext']) ; print_heatmap(populations3[1]['Wext']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
